{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501cc5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib \n",
    "import urllib.robotparser\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import psycopg2\n",
    "import time\n",
    "import hashlib\n",
    "import socket\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "WEB_DRIVER_LOCATION = \"./geckodriver\"\n",
    "TIMEOUT = 5\n",
    "page_types = ('HTML', 'BINARY', 'DUPLICATE', 'FRONTIER')\n",
    "data_types = ('PDF', 'DOC', 'DOCX', 'PPT', 'PPTX')\n",
    "\n",
    "def calculate_hash(data):\n",
    "    hash_object = hashlib.sha256()\n",
    "    hash_object.update(data.encode('utf-8'))\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    return hash_hex\n",
    "\n",
    "\n",
    "def canonicalize(url):\n",
    "    try:\n",
    "        url = urlparse(url)\n",
    "        scheme = url.scheme.lower()\n",
    "        if scheme == 'https':\n",
    "            scheme = 'http'\n",
    "        # host = url.netloc.lower() # .netloc poda tudi port\n",
    "        host = url.hostname.lower() \n",
    "        path = url.path\n",
    "        params = url.params.lower()\n",
    "        query = url.query.lower() \n",
    "        if params != \"\":\n",
    "            paramsList = params.split(';')\n",
    "            paramsList.sort()\n",
    "            params = \"\"\n",
    "            for p in paramsList:\n",
    "                params += \";\" + p\n",
    "        if query != \"\":\n",
    "            queryList = query.split('&')\n",
    "            queryList.sort()\n",
    "            query = \"?\"\n",
    "            first = True\n",
    "            for q in queryList:\n",
    "                if first:\n",
    "                    query += q\n",
    "                    first = False\n",
    "                else:\n",
    "                    query += \"&\" + q\n",
    "        return f\"{scheme}://{host}{path}{params}{query}\"\n",
    "    except:\n",
    "        return \"http://www.gov.si/\"\n",
    "\n",
    "frontier = [\"http://www.gov.si/\", \"http://evem.gov.si/\", \"http://e-uprava.gov.si/\", \"http://e-prostor.gov.si/\"]\n",
    "link_history = []\n",
    "current_hosts = set()\n",
    "images = []\n",
    "firefox_options = FirefoxOptions()\n",
    "firefox_options.add_argument(\"user-agent=fri-ieps-TEST\")\n",
    "firefox_options.add_argument(\"--headless\")\n",
    "\n",
    "\n",
    "def get_robots_content(hostname):\n",
    "    robots_url = f\"http://{hostname}/sitemap\"\n",
    "    robots_r = requests.get(robots_url)\n",
    "    if robots_r.status_code != 200:\n",
    "        parser = urllib.robotparser.RobotFileParser()\n",
    "        parser.parse(robots_r.text)\n",
    "\n",
    "def get_sitemap_content(hostname):\n",
    "    sitemap = requests.get(f\"http://{hostname}/sitemap\")\n",
    "\n",
    "\n",
    "def crawl_page(n, thread):\n",
    "    for _ in range(n):\n",
    "        driver = webdriver.Firefox(executable_path=WEB_DRIVER_LOCATION, options=firefox_options)\n",
    "        host = \"\"\n",
    "        hostname = \"\"\n",
    "        url = \"\"\n",
    "        with lock:\n",
    "            if len(frontier) > 0:\n",
    "                for i in range(len(frontier)): # iščemo stran na IP-ju, ki še ni zaseden\n",
    "                    url = frontier[i]\n",
    "                    if url in link_history:\n",
    "                        continue\n",
    "                    try:\n",
    "                        hostname = urlparse(url).hostname\n",
    "                        host = socket.gethostbyname(hostname)\n",
    "                        if host not in current_hosts:\n",
    "                            current_hosts.add(host)\n",
    "                            frontier.pop(i)\n",
    "                            break\n",
    "                        print(\"Server on \" + url + \" busy (\" + host + \")\")\n",
    "                    except:\n",
    "                        print(\"Wrong URL format\")\n",
    "                        continue\n",
    "                link_history.append(url)\n",
    "                \n",
    "                print(f\"Retrieving web page URL '{url}' ({host}) - thread {thread}\")\n",
    "                driver.get(url)\n",
    "            else:\n",
    "                print(f'----------------empty frontier---------------- thread {thread}')\n",
    "                continue\n",
    "     \n",
    "        time.sleep(TIMEOUT)\n",
    "        current_hosts.remove(host)\n",
    "        \n",
    "        html = driver.page_source\n",
    "        driver.close()\n",
    "\n",
    "        if get_site_id(hostname) is None:\n",
    "            robots_content = get_robots_content(hostname)\n",
    "            sitemap_content = get_sitemap_content(hostname)\n",
    "            insert_new_site(hostname, robots_content, sitemap_content)\n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "        with lock:\n",
    "            for link in soup.find_all('a'):\n",
    "                link = link.get('href')\n",
    "                if link is not None:\n",
    "                    if link.startswith('/'): # stran znotraj domene\n",
    "                        link = canonicalize(\"http://\" + hostname + link)\n",
    "                        if link not in frontier:\n",
    "                            frontier.append(link)\n",
    "                    elif link.startswith('http'): # stran izven domene\n",
    "                        link = canonicalize(link)\n",
    "                        if 'gov.si' in link and link not in frontier:\n",
    "                            frontier.append(link)\n",
    "            for img in soup.find_all('img'):\n",
    "                img = img.get('src')\n",
    "                if img is not None:\n",
    "                    img = url + img\n",
    "                    if img not in images:\n",
    "                        images.append(img);\n",
    "    if thread == 0:\n",
    "        print(\"\\nLink history: \")\n",
    "        for l in link_history:\n",
    "            print(l)\n",
    "        print(\"\\nFrontier: \")\n",
    "        for l in frontier:\n",
    "            print(l)\n",
    "        print(\"\\nImages: \")\n",
    "        for img in images:\n",
    "            print(img)\n",
    "        \n",
    "lock = threading.Lock()\n",
    "database_port = 5431\n",
    "def insert_new_site(domain, robots_content, sitemap_content):\n",
    "    conn = psycopg2.connect(host=\"localhost\", port = database_port, user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "\n",
    "    with lock:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"INSERT INTO crawldb.site (domain, robots_content, sitemap_content) VALUES(%s, %s, %s)\",\n",
    "                    (domain, robots_content, sitemap_content))\n",
    "        cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def get_site_id(domain):\n",
    "    conn = psycopg2.connect(host=\"localhost\", port = database_port, user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT id FROM crawldb.site WHERE domain = %s\", domain)\n",
    "    site_id = cur.fetchone()[0]\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return site_id\n",
    "\n",
    "def insert_page(site_id, page_type_code, url, html_content, https_status_code, accessed_time):\n",
    "    conn = psycopg2.connect(host=\"localhost\", port = database_port, user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "\n",
    "    # If a page is of type HTML, its content should be stored as a value within html_content attribute,\n",
    "    # otherwise (if crawler detects a binary file - e.g. .doc), html_content is set to NULL\n",
    "    # and a record in the page_data table is created\n",
    "    ###\n",
    "    # The duplicate page should not have set the html_content value and should be linked to a duplicate version of a page.\n",
    "    if page_type_code not in page_types:\n",
    "        return\n",
    "    with lock:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"INSERT INTO crawldb.page (site_id, page_type_code, url, html_content, https_status_code, accessed_time) VALUES(%s, %s, %s, %s, %s, %s)\",\n",
    "                    (site_id, page_type_code, url, html_content, https_status_code, accessed_time))\n",
    "        cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def insert_image(page_id, filename, content_type, data, accessed_time):\n",
    "    conn = psycopg2.connect(host=\"localhost\", port = database_port, user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "\n",
    "    # there is no need to populate data field \n",
    "    with lock:\n",
    "        cur = conn.cursor()\n",
    "        if data is not None:\n",
    "            cur.execute(\"INSERT INTO crawldb.image (page_id, filename, content_type, data, accessed_time) VALUES(%s, %s, %s, %s, %s)\",\n",
    "                        (page_id, filename, content_type, data, accessed_time))\n",
    "        else:\n",
    "            cur.execute(\"INSERT INTO crawldb.image (page_id, filename, content_type, accessed_time) VALUES(%s, %s, %s, %s)\",\n",
    "            (page_id, filename, content_type, accessed_time))\n",
    "        cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def insert_link(from_page, to_page):\n",
    "    conn = psycopg2.connect(host=\"localhost\", port = database_port, user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "\n",
    "    with lock:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"INSERT INTO crawldb.link (from_page, to_page)) VALUES(%s, %s)\",\n",
    "                    (from_page, to_page))\n",
    "        cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def insert_page_data(page_id, data_type_code, data):\n",
    "    conn = psycopg2.connect(host=\"localhost\", port = database_port, user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "\n",
    "    # List all other content (.pdf, .doc, .docx, .ppt and .pptx) in the page_data table - there is no need to populate data field \n",
    "    with lock:\n",
    "        cur = conn.cursor()\n",
    "        if data is not None:\n",
    "            cur.execute(\"INSERT INTO crawldb.page_data (page_id, data_type_code, data)) VALUES(%s, %s, %s)\",\n",
    "                        (page_id, data_type_code, data))\n",
    "        else:\n",
    "            cur.execute(\"INSERT INTO crawldb.page_data (page_id, data_type_code)) VALUES(%s, %s)\",\n",
    "                        (page_id, data_type_code))\n",
    "        cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def para(num_workers, pages_per_thread):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        print(f\"\\n ... executing workers ...\\n\")\n",
    "        for i in range(num_workers):\n",
    "            executor.submit(crawl_page, pages_per_thread, i)\n",
    "\n",
    "pages = 25\n",
    "workers = 5\n",
    "pages_per_worker = pages // workers\n",
    "\n",
    "print(f\"Crawling {pages_per_worker * workers} pages with {workers} workers\")\n",
    "para(workers, pages_per_worker)\n",
    "#crawl_page(10,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58c5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
